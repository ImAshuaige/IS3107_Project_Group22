# -*- coding: utf-8 -*-
"""IS3107Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Blx1Si0rQeDPT4gEY3TecnSpCoN-ZCCR
"""

import json
import ast
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
from requests import Request, Session
import pandas as pd
import psycopg2
import io
import os
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import date, datetime, timedelta
from airflow.hooks.postgres_hook import PostgresHook
from tqdm import tqdm
from textblob import TextBlob
import re as re
import nltk
import numpy as np
from nltk.corpus import stopwords
from nltk.stem.porter import *
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import datetime as dt
from sqlalchemy import create_engine
""" from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline """


""" from skforecast.ForecasterAutoreg import ForecasterAutoreg
from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom
from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect
from skforecast.model_selection import grid_search_forecaster
from skforecast.model_selection import backtesting_forecaster
from skforecast.utils import save_forecaster
from skforecast.utils import load_forecaster """


""" from pylab import rcParams
from statsmodels.graphics.tsaplots import plot_predict
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import acf 
import pmdarima as pm """

default_args = {
    'owner': 'airflow',
}

# instantiate DAG
with DAG(
    # DAG NAME
    'IS3107_project',
    default_args=default_args,
    description="",
    schedule_interval='@daily',
    start_date=datetime(2023, 4, 13),
    catchup=False,
    tags=['is3107'],

) as dag:
    # method to transform tweet data
    def transform_data():
        # getting tweets from 2023-03-13 to 2023-04-13
        tweet_directory = "/Users/rachelang/Library/CloudStorage/OneDrive-Personal/IS3107/initial_tweets.json"
        df = pd.read_json(
            tweet_directory, encoding='utf-8', lines=True)
        # df = pd.read_json(tweet_directory, lines=True)
        print("df: ")
        print(df.to_string)
        df = df.drop(columns=['_type', 'rawContent', 'id', 'replyCount', 'cashtags', 'retweetCount', 'card', 'vibe',
                              'outlinks', 'content', 'outlinksss', 'tcooutlinks', 'tcooutlinksss', 'quoteCount',
                              'conversationId', 'source', 'sourceUrl', 'quotedTweet', 'inReplyToTweetId',
                              'inReplyToUser', 'mentionedUsers', 'coordinates', 'place', 'links',
                              'media'])  # drop useless cols
        # print(df) #check
        df.drop(df[df.lang != 'en'].index, inplace=True)  # only english tweets
        # print(df) #check
        df = pd.concat(
            [df, df.pop("user").apply(pd.Series).add_prefix("user_")], axis=1
        )  # flatten df (user)
        # print(df) #check
        df = df.drop(columns=['user__type', 'user_protected', 'user_link', 'user_profileImageUrl', 'user_profileBannerUrl',
                              'user_label', 'user_descriptionUrls', 'user_linkTcourl', 'user_linkUrl', 'user_url',
                              'user_mediaCount', 'user_username', 'user_statusesCount', 'user_favouritesCount',
                              'user_listedCount', 'user_mediaCount', 'user_descriptionLinks', 'user_rawDescription',
                              'user_renderedDescription', 'url'])  # drop more cols
        df.replace('\t', ' ', regex=True, inplace=True)
        df.replace('\n', ' ', regex=True, inplace=True)
        df.replace('\r', ' ', regex=True, inplace=True)
        df.replace('\v', ' ', regex=True, inplace=True)
        df.replace('\b', ' ', regex=True, inplace=True)
        df.replace('\a', ' ', regex=True, inplace=True)
        df.replace('\f', ' ', regex=True, inplace=True)
        # remove latin characters
        df.replace('\u0000', '', regex=True, inplace=True)
        df.replace({r'[^\x00-\x7F]+': ''}, regex=True, inplace=True)
        df.replace('\xfc', '', regex=True, inplace=True)

        df.drop(df[df.lang != 'en'].index, inplace=True)  # only english tweets
        df['date'] = pd.to_datetime(
            df['date'], format='%Y-%m-%dT%H:%M:%S')

        df_reordered = df[['username', 'user_location', 'user_description', 'user_followersCount',
                          'user_friendsCount', 'user_verified', 'date', 'renderedContent', 'hashtags']]
        print("df_reordered: ")
        print(df_reordered)  # final df

        # check curr data
        alchemyEngine = create_engine(
            'postgresql+psycopg2://postgres:password@localhost:5432/IS3107_Project')
        dbConnection = alchemyEngine.connect()
        tweet_df_prev = pd.read_sql(
            "select * from public.bitcoin_tweets", dbConnection)
        pd.set_option('display.expand_frame_repr', False)

        count = 0
        df_reordered.insert(9, 'tweet_id', range(
            count + 1, count + 1 + len(df_reordered)))
        df_reordered = df_reordered.dropna()

        # prevent replicas
        df_reordered.drop(df_reordered[df_reordered['tweet_id'].isin(
            tweet_df_prev['tweet_id'])].index, inplace=True)

        # df_reordered['renderedContent'] = df_reordered['renderedContent'].apply(
        #    ast.literal_eval).str.decode("utf-8")
        df_reordered['renderedContent'] = df_reordered['renderedContent'].apply(
            lambda x: x).str.decode("utf-8")

        task_data_upload('bitcoin_tweets', df_reordered)

        # Read data from PostgreSQL database table and load into a DataFrame instance
        dataFrame2 = pd.read_sql(
            "select * from public.bitcoin_prices", dbConnection)
        pd.set_option('display.expand_frame_repr', False)
        # Close the database connection
        dbConnection.close()

        # transform bitcoin data
        bitcoin_directory = "/Users/rachelang/Library/CloudStorage/OneDrive-Personal/IS3107/bitcoin.csv"
        df2 = pd.read_csv(bitcoin_directory)  # Increased from 300 to 100000
        df2 = df2.dropna()
        df2 = df2.drop(columns=['coin_name'], axis=1)
        df2['price_id'] = df2.index  # add id here
        df2.drop(df2[df2['price_id'].isin(
            dataFrame2['price_id'])].index, inplace=True)
        df2['date'] = pd.to_datetime(
            df2['date'], format='%Y-%m-%dT%H:%M:%S')
        task_data_upload("bitcoin_prices", df2)

    def task_data_upload(table_name, data):
        # engine to connect to postgres
        engine = create_engine(
            'postgresql+psycopg2://postgres:password@localhost:5432/IS3107_Project', encoding='utf-8')
        conn = engine.raw_connection()
        cur = conn.cursor()

        # data = data.apply(lambda x: x.encode('utf-8')
        #                  if x.dtype == 'object' else x)

        output = io.StringIO()
        # convert data to CSV format
        data.to_csv(output, sep='\t', header=False,
                    index=False, encoding='utf-8')
        output.seek(0)
        contents = output.getvalue()
        cur.copy_from(output, table_name)
        conn.commit()
        cur.close()
        conn.close()

    # code to generate table
    def create_bitcoin_tables():
        # Connect to the PostgreSQL database
        conn = psycopg2.connect(
            host="localhost",
            database="IS3107_Project",
            user="postgres",
            password="password"
        )

        # Create a cursor object
        cur = conn.cursor()

        # Table 1: Bitcoin Table
        # price_id SERIAL PRIMARY KEY
        sql1 = """
            CREATE TABLE IF NOT EXISTS bitcoin_prices (
              date TIMESTAMP,
              price NUMERIC,
              total_volume NUMERIC,
              market_cap NUMERIC,
              price_id SERIAL PRIMARY KEY
            );
        """

        # Table 2: Tweet Table
        sql2 = """
            CREATE TABLE IF NOT EXISTS bitcoin_tweets (
              user_name TEXT,
              user_location TEXT,
              user_description TEXT,
              user_followers DECIMAL,
              user_friends TEXT,
              user_verified BOOLEAN,
              date TIMESTAMP,
              text TEXT,
              hashtags  TEXT,
              tweet_id SERIAL PRIMARY KEY
            );   
        """

        # Execute the SQL statement
        cur.execute(sql1)
        cur.execute(sql2)

        # Commit the changes to the database
        conn.commit()

        # Close the cursor and the database connection
        cur.close()
        conn.close()

    def sentiment_analysis():
        alchemyEngine = create_engine(
            'postgresql+psycopg2://postgres:password@localhost:5432/IS3107_Project')
        dbConnection = alchemyEngine.connect()

        # select required columns from bitcoin_tweets table
        selected_columns = ['tweet_id', 'date', 'text']
        dataset = pd.read_sql("SELECT {} FROM public.bitcoin_tweets".format(
            ", ".join(selected_columns)), dbConnection)
        dataset['text'] = dataset['text'].apply(cleanEach)
        cleanWords = []
        for item in tqdm(dataset['text']):
            words = toWords(item)
            cleanWords += [words]

        dataset['text'] = cleanWords

        dataset = compute_vader_scores(dataset, 'text')
        sentiment_task_data_upload("bitcoin_tweets_sentiment", dataset)

    def create_sentiment_table():
        # Connect to the PostgreSQL database
        conn = psycopg2.connect(
            host="localhost",
            database="IS3107_Project",
            user="postgres",
            password="password"
        )

        # Create a cursor object
        cur = conn.cursor()
        sql = """
            CREATE TABLE IF NOT EXISTS bitcoin_tweets_sentiment (
            tweet_id INTEGER,
            date DATE,
            text TEXT,
            negative NUMERIC,
            neutral NUMERIC,
            positive NUMERIC,
            compound NUMERIC
            );
        """

        # Execute the SQL statement
        cur.execute(sql)
        # Commit the changes to the database
        conn.commit()

        # Close the cursor and the database connection
        cur.close()
        conn.close()

    # create a function to clean the tweets
    def cleanEach(twt):
        # removes the '#' from bitcoin
        twt = re.sub("#bitcoin", 'bitcoin', twt)
        # removes the '#' from Bitcoin
        twt = re.sub("#Bitcoin", 'Bitcoin', twt)
        twt = re.sub('#[A-Za-z0-9]+', '', twt)  # removes any string with a '#'
        twt = re.sub('\\n', '', twt)  # removes the '\n' string
        twt = re.sub('https:\/\/\S+', '', twt)  # removes any hyperlinks
        twt = re.sub(r"[^a-zA-Z0-9]", " ", twt)  # removes any non-words
        return twt

    def toWords(twt):
        # standardise lower case
        txt = twt.lower()
        # tokenize words
        words = txt.split()
        # remove the stopwords from text
        words = [
            word for word in words if word not in stopwords.words("english")]
        # stemming process
        words = [PorterStemmer().stem(word) for word in words]
        # return list of processed words
        return words

    def unlist(list):
        words = ''
        for item in list:
            words += item+' '
        return words

    def bitcoin_stream():
       # gets latest quotes for BTC from coinmarketcap

        url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'
        parameters = {
            'id': '1',  # instead of id we can also use 'symbol':'BTC'
            'convert': 'USD'
        }
        headers = {
            'Accepts': 'application/json',
            'Accept-Encoding': 'deflate, gzip',  # to make data load faster
            'X-CMC_PRO_API_KEY': 'bc4a2eea-7e9c-45d2-b028-99bbde10b44b',
            # apikey here (mine has limit of ~300 calls/day)
        }

        session = Session()
        session.headers.update(headers)

        try:
            response = session.get(url, params=parameters)
            data = json.loads(response.text)  # data-json
            print(data)
        except (ConnectionError, Timeout, TooManyRedirects) as e:
            print(e)

        # Time is in ISO 8601 format
        print("date: " + data['data']['1']['last_updated'])
        print("price: " + str(data['data']['1']
              ['quote']['USD']['price']))  # in USD
        print("total value of BTC traded in the last 24h: " +
              str(data['data']['1']['quote']['USD']['volume_24h']))

        # converting into pandas
        df = pd.DataFrame(data["data"]).T
        df = df.drop(columns="tags")

        df = pd.concat(
            [df, df.pop("platform").apply(pd.Series).add_prefix("platform_")], axis=1
        )
        df = pd.concat(
            [df, df.pop("quote").apply(pd.Series).add_prefix("quote_")], axis=1
        )
        df = pd.concat(
            [df, df.pop("quote_USD").apply(pd.Series).add_prefix("quote_USD_")], axis=1
        )

        df = df.drop(columns=["quote_USD_tvl", "quote_USD_last_updated", "name", "num_market_pairs",
                              "self_reported_circulating_supply", "is_active", "cmc_rank", "is_fiat", "slug",
                              "self_reported_market_cap", "tvl_ratio", "circulating_supply", "date_added", "id",
                              "max_supply", "symbol", "total_supply", "quote_USD_volume_change_24h",
                              "quote_USD_percent_change_7d", "quote_USD_percent_change_30d",
                              "quote_USD_percent_change_60d", "quote_USD_percent_change_90d",
                              "quote_USD_market_cap_dominance", "quote_USD_fully_diluted_market_cap",
                              "quote_USD_percent_change_1h", "quote_USD_percent_change_24h", "infinite_supply"])

        df['last_updated'] = pd.to_datetime(df['last_updated'],
                                            format="%Y-%m-%dT%H:%M:%S.%f")
        # seems like there is a 2min lag betw latest and when you collect the data
        df['last_updated'] = df['last_updated'].dt.tz_convert(
            'Asia/Singapore')  # change tz to sg

        df = df.dropna()

        print("bitcoin prices df: ")
        print(df)

        alchemyEngine = create_engine(
            'postgresql+psycopg2://postgres:password@localhost:5432/IS3107_Project')
        dbConnection = alchemyEngine.connect()
        df2 = pd.read_sql(
            "select * from public.bitcoin_prices", dbConnection)
        pd.set_option('display.expand_frame_repr', False)
        dbConnection.close()
        if (df2.empty):
            count = 0
            df.insert(4, 'price_id', range(count + 1, count + 1 + len(df)))
            df['last_updated'] = pd.to_datetime(df['last_updated']).dt.date
        else:
            lastrow = df2.iloc[-1]
            count = lastrow['price_id']
            df.insert(4, 'price_id', range(count + 1, count + 1 + len(df)))
            df['last_updated'] = pd.to_datetime(df['last_updated']).dt.date
            latest_date = lastrow['date']
            df.drop(df[df.last_updated == latest_date].index, inplace=True)

        task_data_upload('bitcoin_prices', df)

    def tweets_stream():
        # Returns the current local date
        today = date.today().strftime("%Y-%m-%d")
        yesterday = date.today() - timedelta(days=1)
        yesterday = yesterday.strftime("%Y-%m-%d")

        # remove --max-results 1000 to get all tweets, but it was running too slowly so i limited it
        query = "snscrape --jsonl --progress --max-results 1000 --since " + yesterday + \
            " twitter-search 'bitcoin until:" + today + "' > tweets_scraped.json"

        home_directory = os.path.expanduser('~')
        os.system(query)
        # ive checked that my json file creates in my home directory, check w others too
        df = pd.read_json(home_directory + '/tweets_scraped.json', lines=True)
        print("df: ")
        print(df.to_string)
        df = df.drop(columns=['_type', 'rawContent', 'id', 'replyCount', 'cashtags', 'retweetCount', 'card', 'vibe',
                              'outlinks', 'content', 'outlinksss', 'tcooutlinks', 'tcooutlinksss', 'quoteCount',
                              'conversationId', 'source', 'sourceUrl', 'quotedTweet', 'inReplyToTweetId',
                              'inReplyToUser', 'mentionedUsers', 'coordinates', 'place', 'links',
                              'media'])  # drop useless cols
        # print(df) #check
        df.drop(df[df.lang != 'en'].index, inplace=True)  # only english tweets
        # print(df) #check
        df = pd.concat(
            [df, df.pop("user").apply(pd.Series).add_prefix("user_")], axis=1
        )  # flatten df (user)
        # print(df) #check
        df = df.drop(columns=['user__type', 'user_protected', 'user_link', 'user_profileImageUrl', 'user_profileBannerUrl',
                              'user_label', 'user_descriptionUrls', 'user_linkTcourl', 'user_linkUrl', 'user_url',
                              'user_mediaCount', 'user_username', 'user_statusesCount', 'user_favouritesCount',
                              'user_listedCount', 'user_mediaCount', 'user_descriptionLinks', 'user_rawDescription',
                              'user_renderedDescription', 'url'])  # drop more cols
        df.replace('\t', ' ', regex=True, inplace=True)
        df.replace('\n', ' ', regex=True, inplace=True)
        df.replace('\r', ' ', regex=True, inplace=True)
        df.drop(df[df.lang != 'en'].index, inplace=True)  # only english tweets
        df_reordered = df[['username', 'user_location', 'user_description', 'user_followersCount',
                          'user_friendsCount', 'user_verified', 'date', 'renderedContent', 'hashtags']]
        print("df_reordered: ")
        print(df_reordered)  # final df
        alchemyEngine = create_engine(
            'postgresql+psycopg2://postgres:password@localhost:5432/IS3107_Project')
        dbConnection = alchemyEngine.connect()
        df2 = pd.read_sql(
            "select * from public.bitcoin_tweets", dbConnection)
        pd.set_option('display.expand_frame_repr', False)
        dbConnection.close()
        if (df2.empty):
            count = 0
            df_reordered.insert(9, 'tweet_id', range(
                count + 1, count + 1 + len(df_reordered)))
        else:
            lastrow = df2.iloc[-1]
            count = lastrow['tweet_id']
            print("tweets stream last count: ")
            print(count)
            df_reordered.insert(9, 'tweet_id', range(
                count + 1, count + 1 + len(df_reordered)))

        df_reordered = df_reordered.dropna()

        task_data_upload('bitcoin_tweets', df_reordered)

    def compute_vader_scores(dataset, label):
        sid = SentimentIntensityAnalyzer()
        dataset['negative'] = dataset[label].apply(
            lambda x: sid.polarity_scores(unlist(x))["neg"])
        dataset['neutral'] = dataset[label].apply(
            lambda x: sid.polarity_scores(unlist(x))["neu"])
        dataset['positive'] = dataset[label].apply(
            lambda x: sid.polarity_scores(unlist(x))["pos"])
        dataset['compound'] = dataset[label].apply(
            lambda x: sid.polarity_scores(unlist(x))["compound"])
        dataset['text'] = dataset[label].apply(lambda x: unlist(x))
        return dataset

    def sentiment_task_data_upload(table_name, data):
        # engine to connect to postgres
        engine = create_engine(
            'postgresql+psycopg2://postgres:password@localhost:5432/IS3107_Project')
        conn = engine.raw_connection()

        dataframe = pd.read_sql(
            "select * from public.bitcoin_tweets_sentiment", conn)
        pd.set_option('display.expand_frame_repr', False)
        print(dataframe.head(5))

        data.drop(data[data['tweet_id'].isin(
                  dataframe['tweet_id'])].index, inplace=True)

        cur = conn.cursor()
        output = io.StringIO()
        # convert data to CSV format
        data.to_csv(output, sep='\t', header=False, index=False)
        output.seek(0)
        contents = output.getvalue()
        cur.copy_from(output, table_name)
        conn.commit()
        cur.close()
        conn.close()

    # This functions have not been used yet
    def calculate_positive_overall(positive, negative):
        if positive > negative:
            return 1
        else:
            return 0

    # This functions have not been used yet
    def calculate_negative_overall(positive, negative):
        if negative > positive:
            return 1
        else:
            return 0

create_bitcoin_tables = PythonOperator(
    task_id="create_bitcoin_tables", python_callable=create_bitcoin_tables, dag=dag)
transform_data = PythonOperator(
    task_id="transform_data", python_callable=transform_data, dag=dag)
create_sentiment_table = PythonOperator(
    task_id="create_sentiment_table", python_callable=create_sentiment_table, dag=dag)
bitcoin_stream = PythonOperator(
    task_id="bitcoin_stream", python_callable=bitcoin_stream, dag=dag)
tweets_stream = PythonOperator(
    task_id="tweets_stream", python_callable=tweets_stream, dag=dag)
sentiment_analysis = PythonOperator(
    task_id="sentiment_analysis", python_callable=sentiment_analysis, dag=dag)

create_bitcoin_tables >> transform_data >> create_sentiment_table >> bitcoin_stream >> tweets_stream >> sentiment_analysis
